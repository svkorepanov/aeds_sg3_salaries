# -*- coding: utf-8 -*-
"""ADES_WG3_assignment_v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RpRj1w7Sj-Xxj7RWo9Br-XaOT_eHNJ-0

Imports
"""

import os
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

os.getcwd()

"""# Understanding the Dataset

Let's look at the first few rows of the dataset
Even from this small sample we can see a lot of missing data and data that is not relevant.

For example some of the job postings are missing `salary data` which is very important for our project.

"""

df = pd.read_csv('/content/sample_data/us-software-engineer-jobs-zenrows.csv')
df.head()

"""## Information about data set

From the information we are able to see all type of data in our disposal.
Let list a few of them that are more important to us and describe their meaning



*   **Title** - title of the job position on which company is hiring
*   **Company** - name of the company that is hiring
*   **Salary** - Salary range that the company is offering for the position
*   **Location** - Location where the office of the company is located
*   **Types** - Type of employment: Full-time, Part-time, Contract, Internship, Temporary

Other data points are considered unrelated for our goals. Therefore, let's remove them.


"""

df.info()
columns_to_drop = ['rating', 'review_count', 'relative_time', 'hires_needed', 'hires_needed_exact',
                   'urgently_hiring', 'remote_work_model', 'snippet', 'dradis_job', 'link', 'new_job',
                   'job_link', 'sponsored', 'featured_employer', 'indeed_applyable', 'ad_id', 'remote_location',
                   'source_id', 'hiring_event_job', 'indeed_apply_enabled', 'job_location_postal', 'company_overview_link',
                   'activity_date', 'location_extras']
df_cleaned = df.drop(columns_to_drop, axis=1)

"""## Cleaned Data Set
Here is the data remained after cleaning
"""

df_cleaned.head()

"""## Normalising Data
After we cleaned irrelevant data we need to normalise salary ranges. In the code snippet bellow we can see that salary range for a position can be stated as "a year", "a month", "a week", "an hour"

- First of all we will convert salary data to dollars per year.

- And after we will split range data to min_salary and max_salary for the role

### Converting Salary Data
let's look at the unique values in the 'salary' column to understand the data better
"""

# Get unique values after dropping NaNs
unique_salaries = set(df_cleaned['salary'].unique())
unique_salaries_df = pd.DataFrame(unique_salaries)
unique_salaries_df.head(n=100)

"""### Converting to Yearly Salary
From previous snippet we see that the salary appears in different formats and rates.
- a year, a month, a week, an hour
- range and fixed rate

- Let's try to convert all of them to yearly salary
"""

# Cleaning symbols
def clean_text(text: str):
    if pd.isna(text):
        return "NaN"
    # Split the text by ' - ' to separate the salary range
    cleaned_text = (text
                    .lower()
                    .replace('$', '')
                    .replace(',', '')
                    .replace(' - ', ' '))

    return cleaned_text

df_cleaned['salary_temp'] = df_cleaned['salary'].apply(clean_text)

df_cleaned['salary_temp']

# Define the assumption for hours worked per year
hours_per_year = 1892  # Statistical data from clockify

def convert_range_match(match: re.Match[str]) -> object:
    if match.group(3) == 'a year':
        yearly_min = float(match.group(1))
        yearly_max = float(match.group(2))
    if match.group(3) == 'a month':
        yearly_min = float(match.group(1)) * 12
        yearly_max = float(match.group(2)) * 12
    if match.group(3) == 'a week':
        yearly_min = float(match.group(1)) * 52
        yearly_max = float(match.group(2)) * 52
    if match.group(3) == 'a day':
        yearly_min = float(match.group(1)) * 260
        yearly_max = float(match.group(2)) * 260
    if match.group(3) == 'an hour':
        yearly_min = float(match.group(1)) * hours_per_year
        yearly_max = float(match.group(2)) * hours_per_year
    return f"{yearly_min:.0f} {yearly_max:.0f}"

def convert_fixed_match(match: re.Match[str]) -> object:
    if match.group(2) == 'a year':
        yearly_min = float(match.group(1))
        yearly_max = yearly_min
    if match.group(2) == 'a month':
        yearly_min = float(match.group(1)) * 12
        yearly_max = yearly_min
    if match.group(2) == 'a week':
        yearly_min = float(match.group(1)) * 52
        yearly_max = yearly_min
    if match.group(2) == 'a day':
        yearly_min = float(match.group(1)) * 260
        yearly_max = yearly_min
    if match.group(2) == 'an hour':
        yearly_min = float(match.group(1)) * hours_per_year
        yearly_max = yearly_min
    return f"{yearly_min:.0f} {yearly_max:.0f}"

def converter(salary: str):
    if salary == 'NaN':
        return salary

    range_pattern = r'(\d+\.?\d*) (\d+\.?\d*) (a year|a month|an hour|a week|a day)'
    fixed_pattern = r'(\d+\.?\d*) (a year|a month|an hour|a week|a day)'

    range_match = re.search(range_pattern, salary)
    fixed_match = re.search(fixed_pattern, salary)

    if range_match:
        return convert_range_match(range_match)

    if fixed_match:
        return convert_fixed_match(fixed_match)
# Apply the function to the 'salary' column
df_cleaned['salary_temp'] = df_cleaned['salary_temp'].apply(converter)
df_cleaned['salary_temp']

salary_split = df_cleaned['salary_temp'].str.split(expand=True)

# Assigning the split parts to 'min_salary' and 'max_salary' columns
df_cleaned['min_salary'] = salary_split[0]
df_cleaned['max_salary'] = salary_split[1]

df_cleaned.drop(columns='salary_temp', inplace=True)
dataframe = df_cleaned.dropna(subset="salary")
dataframe

"""## Descriptive analysis"""

df_copy = dataframe.copy()
df_copy['min_salary'] = df_copy['min_salary'].astype(int, copy=True)
df_copy['max_salary'] = df_copy['max_salary'].astype(int, copy=True)

df_copy.describe()

"""### Most frequent numbers"""

df_copy.mode()

"""### Variance"""

df_copy.var(numeric_only=True).astype(int, copy=True)

"""### Standard deviation"""

df_copy.std(numeric_only=True).astype(int, copy=True)

"""### Range"""

df_copy.max(numeric_only=True) - df_copy.min(numeric_only=True)

"""### Interquartile Range"""

df_copy.quantile(0.75, numeric_only=True) - df_copy.quantile(0.25, numeric_only=True)

"""### Distribution analysis"""

plt.figure(figsize=(8, 6))
plt.hist(df_copy['min_salary'], bins=10, color='red', edgecolor='black', stacked=True)
plt.hist(df_copy['max_salary'], bins=10, color=(0.0, 0.5, 0.5, 0.7), edgecolor='black', stacked=True)
plt.title('Distribution of Salaries')
plt.xlabel('Salary')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""### Frequency table"""

# Frequency table of salaries
min_salaries = pd.cut(df_copy['min_salary'], bins=10, precision=0)
frequency_table = pd.Series(min_salaries).value_counts().sort_index()
print("Min Salary frequency Table:")
print(frequency_table)

max_salaries = pd.cut(df_copy['max_salary'], bins=10, precision=0)
frequency_table = pd.Series(max_salaries).value_counts().sort_index()
print("\nMax Salary frequency Table:")
print(frequency_table)

"""### Box plot of salaries by location

First, let's normalise the 'location' column by creating a function that would only keep the state.
"""

# Function to extract state abbreviation from location
def extract_state(location):
    if location.lower() == 'remote':
        return 'Remote'
    else:
        match = re.search(r',\s*([A-Z]{2})$', location)
        if match:
            return match.group(1)
        else:
            return 'Unknown'

# Apply the function to create a new column 'state'
df_copy['state'] = df_copy['location'].apply(extract_state)

# Box plot of salaries by state
plt.figure(figsize=(10, 6))
sns.boxplot(x='state', y='min_salary', data=df_copy, palette='pastel', hue='state', legend=False)
plt.title('Box Plot of Salaries by State')
plt.xlabel('State')
plt.ylabel('Salary')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

"""### Correlation coefficient"""

location_mapping = {location: i for i, location in enumerate(df_copy['location'].unique())}
df_copy['location_encoded'] = df_copy['location'].map(location_mapping)

# Compute correlation coefficient
correlation_coefficient = df_copy['min_salary'].corr(df_copy['location_encoded'])

print("correlation Coefficient:", correlation_coefficient)








